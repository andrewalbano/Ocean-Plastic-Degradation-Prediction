# Ocean Plastic Degradation Rate Prediction Algorithms
 Optimistic promises of the benefits of plastics have largely come to fruition with their set of diverse properties and versatility, however, the challenge of waste management, particularly the accumulation of plastic in the ocean, has gained traction as a growing global challenge. Building on [previous research](https://www.nature.com/articles/s41467-020-14538-z), we seek to compare various machine learning algorithms for classifying the rate of degradation of plastic polymers in the ocean using only their physical properties. We narrowed down our features to a list of 6 candidates by considering correlation matrices, standard t-tests for significance, and considering the linear seperability of the features. We standardize the data and implement oversampling techniques to address bias in the dataset. We split the data into training, validation, and test sets. We attempted to optimize the hyperparameters using 3-fold cross validation. In feature pairs, we implemented linear svm models, svm models with an rbf kernel, svm models with a polynomial kernel, and neural networks. The decision boundary for each feature pair with optimized hyperparameters was compared to the ground truth and can be seen in the results folder. We are currently preparing additional results that track the accuracy of the test and validation sets while varying the hyperparameters. A paper on our findings is included in our repo.

# linear_separability_analysis.py
This was used to check if any feature pair was linearly seperable and identify trends related to each feature pair. Using a defined feature list and a target (3-tier rank) it creates a subset of the data and screens it by removing rows that are missing data in the feature columns. It then plots all feature pairs against eachother and labels each point based on the 3-tier degradation rank. All plots are saved in the [linear_seperability_plots](https://github.com/andrewalbano/Ocean-Plastic-Degradation-Prediction/tree/main/linear_separability_plots) folder. A sample result is shown below for logP/SA vs all features.
<img src="linear_separability_plots\linear_separability_plot_LogPSA_vs_features.png" alt="sample result" />

# correlationMatrices.py
This checks for correlation between the features and 2 weight loss reporting methods, BOD %/day, and weight loss %/day (referred to as the targets). It outputs a correlation matrix heatmap for each reporting method and an excel sheet summarizing the results of a significance test which was used to narrow down feature selection. Using the defined feature list and target it creates a subset of the data and screens it by removing rows that are missing data in the feature and target columns. The subset is used to create a correlation matrix. A standard t-test to check the significance of each Pearson correlation coefficient with respect to the target column using a specified significance level (alpha = 0.1) is completed. The results can be found in the [correlation_significance_results](https://github.com/andrewalbano/Ocean-Plastic-Degradation-Prediction/tree/main/correlation_significance_results) folder 

# SVM.py
The user is given a choice of which kernel type to use. A subset of the data is created using the desired features and target (3-tier rank) and applies listwise deletion (deletes rows if any feature is missing). Class imbalances are handled using SMOTE and the features are standardized using StandardScaler. The data is split into training, validation, and test sets. 3-fold cross validation is used for validation. Hyperparameters are given by param_grid and tuned (note that gamma = 'scale'). GridSearchCV is used to determine the best model for each feature pair which is saved in SVM_"KERNEL"_results. We use the default scoring method, accuracy.score to determine the best estimator. Once the best hyperparameters are chosen using GridSearchCV, the model is refit using the entire training set. We report the training accuracy (using the entire training set) and test accuracy based on this refit model. A classification report for each feature pair's best model is saved in an excel with the hyper parameters used. The decision boundary is plotted along with the ground truth for the training set and test set. A sample result is shown below.
<img src="SVM_rbf_results\SVM_rbf_plot_Tg_vs_LogPSA.png" alt="sample result" />

# SVM_hyperparams.py
The user is given a choice of which kernel type to use. A subset of the data is created using the desired features and target (3-tier rank) and applies listwise deletion (deletes rows if any feature is missing). Class imbalances are handled using SMOTE and the features are standardized using StandardScaler. The data is split into training, validation, and test sets. 3-fold cross validation is used for validation. Hyperparameters are given by param_grid and tuned (note that gamma = 'scale'). GridSearchCV is used to determine the best model for each feature pair which is saved in SVM_"KERNEL"_results_hyperparam_testing. We use the default scoring method, accuracy.score to determine the best estimator. Once the best hyperparameters are chosen using GridSearchCV, the model is refit using the entire training set. We report the training accuracy (using the entire training set) and test accuracy based on this refit model. A classification report for each feature pair's best model is saved in an excel with the hyper parameters used. We also include some results of the grid search testing process in the excel sheet.The decision boundary is plotted along with the ground truth for the training set and test set. A sample result is shown below.
<img src="SVM_rbf_results_hyperparam_testing\SVM_rbf_plot_enthalpy_vs_Tg.png" alt="sample result" />
